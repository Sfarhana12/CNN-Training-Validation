{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85be__3ptBRM"
      },
      "outputs": [],
      "source": [
        "#New_Training - Validation - Testing\n",
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "from operator import add, sub\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score,confusion_matrix,roc_auc_score\n",
        "\n",
        "# This function lodes the traning dataset and divide it into two non-overlapping validation and traning datasets\n",
        "\n",
        "def get_train_valid_loader(data_dir,\n",
        "                           batch_size,\n",
        "                           random_seed,\n",
        "                           kfolde=10,\n",
        "                           shuffle=True,\n",
        "                           show_sample=False,\n",
        "                           num_workers=1,\n",
        "                           pin_memory=False,\n",
        "                           shuffelthevaluditation=1):\n",
        "    \"\"\"\n",
        "\n",
        "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
        "    Params\n",
        "    ------\n",
        "    - data_dir: path directory to the dataset.\n",
        "    - batch_size: how many samples per batch to load.\n",
        "      mentioned in the paper. Only applied on the train split.\n",
        "    - random_seed: fix seed for reproducibility.\n",
        "    - kfolde: the number of foldes in cross validation\n",
        "      the validation set. Should be a float in the range [0, 1].\n",
        "    - shuffle: whether to shuffle the train/validation indices.\n",
        "    - show_sample: plot  sample grid of the dataset.\n",
        "    - num_workers: number of subprocesses to use when loading the dataset.\n",
        "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
        "      True if using GPU.\n",
        "    -shuffelthevaluditation: the folding index of validation set\n",
        "    Returns\n",
        "    -------\n",
        "    - train_loader: training set iterator.\n",
        "    - valid_loader: validation set iterator.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # load the dataset\n",
        "    train_x = sorted(glob(os.path.join(data_dir, \"image\", \"*.png\")))[:1536]\n",
        "    train_y = sorted(glob(os.path.join(data_dir, \"mask\", \"*.png\")))[:1536]\n",
        "\n",
        "    valid_x = sorted(glob(os.path.join(data_dir, \"image\", \"*.png\")))[1536:]\n",
        "    valid_y = sorted(glob(os.path.join(data_dir, \"mask\", \"*.png\")))[1536:]\n",
        "\n",
        "\n",
        "    train_dataset = DriveDataset(train_x, train_y,augmentation=None)\n",
        "    valid_dataset = DriveDataset(valid_x, valid_y,augmentation=None)\n",
        "    #get_validation_augmentation()\n",
        "\n",
        "    print(len(train_x),len(train_y),len(valid_x),len(valid_y))\n",
        "\n",
        "    dataset=ConcatDataset([train_dataset, valid_dataset])\n",
        "\n",
        "    valid_size=(len(dataset)/kfolde)/(len(dataset))\n",
        "    num_train = len(dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[:split* (int( shuffelthevaluditation) -1 )] + indices[split*int( shuffelthevaluditation):], indices[split * (int( shuffelthevaluditation) -1 ):split * int( shuffelthevaluditation)]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    data_str = f\"Dataset Size:\\nTrain sampler: {len(train_sampler)} - Validsampler: {len(valid_sampler)}\\n\"\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, sampler=valid_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "#best val accuracy\n",
        "#written to CSV file\n",
        "#K fold Cross Validation\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Ground truth \"\"\"\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "    y_true = y_true.reshape(-1)\n",
        "\n",
        "    \"\"\" Prediction \"\"\"\n",
        "    y_pred = y_pred.cpu()\n",
        "    y_pred = y_pred.detach().numpy()\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "\n",
        "    cm1 = confusion_matrix(y_true,y_pred)\n",
        "    total1 = sum(sum(cm1))\n",
        "\n",
        "    #TN (True Negative) = cm1[0,0]\n",
        "    #FP (False Positive) = cm1[0,1]\n",
        "    #FN (False Negative) = cm1[1,0]\n",
        "    #TP (True Positive) = cm1[1,1]\n",
        "\n",
        "    acc = (cm1[0,0]+cm1[1,1])/total1\n",
        "    fdr = cm1[0,1]/(cm1[0,1]+cm1[1,1])   #False Detection Rate (FDR) = FP / (FP + TP)\n",
        "    recall = cm1[1,1]/(cm1[1,1]+cm1[1,0])   # Recall = TP/ (TP + FN)\n",
        "    precision = cm1[1,1] /(cm1[1,1]+cm1[0,1])  # Precision = TP/ (TP + FP)\n",
        "    f1 = (2*(precision*recall))/(precision + recall)\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1]) # Specivity = TN / (TN + FP)\n",
        "    Sensitivity =  cm1[1,1]/(cm1[1,1]+cm1[1,0]) # Sensitivity = TP / (TP + FN)\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    intersection = np.logical_and(y_true==1, y_pred==1)\n",
        "    union = np.logical_or(y_true==1, y_pred==1)\n",
        "\n",
        "    iou = np.sum(intersection) / np.sum(union)\n",
        "    dice_coeff = (2. * intersection.sum()) / (y_true.sum() + y_pred.sum())\n",
        "    g_mean = math.sqrt(Specificity * Sensitivity)\n",
        "    pe =((cm1[1,1] + cm1[1,0])*(cm1[1,1] + cm1[0,1]) + (cm1[0,0]  + cm1[0,1])*(cm1[0,0]  + cm1[1,0]))/((cm1[1,1] + cm1[0,0]  + cm1[0,1] + cm1[1,0])**2)\n",
        "    kappa_score = (acc - pe)/(1 - pe)\n",
        "\n",
        "\n",
        "\n",
        "    return [ acc, fdr, recall, precision,  f1, Specificity, Sensitivity, auc, iou, dice_coeff, g_mean, kappa_score]\n",
        "\n",
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, dtype=torch.float32)\n",
        "        y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        score = calculate_metrics(y, y_pred)\n",
        "        metrics_score = list(map(add, metrics_score, score))\n",
        "    epoch_loss = epoch_loss/len(loader)\n",
        "    metrics_score = [sc / len(loader) for sc in metrics_score]\n",
        "\n",
        "    return epoch_loss, metrics_score\n",
        "\n",
        "def evaluate(model, loader, loss_fn, device,mode):\n",
        "    epoch_loss = 0.0\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "            score = calculate_metrics(y, y_pred)\n",
        "            metrics_score = list(map(add, metrics_score, score))\n",
        "        epoch_loss = epoch_loss/len(loader)\n",
        "        metrics_score = [sc / len(loader) for sc in metrics_score]\n",
        "    return epoch_loss, metrics_score\n",
        "\n",
        "def test(model, loader, loss_fn, device,mode):\n",
        "    epoch_loss = 0.0\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "            score = calculate_metrics(y, y_pred)\n",
        "            metrics_score = list(map(add, metrics_score, score))\n",
        "        epoch_loss = epoch_loss/len(loader)\n",
        "        metrics_score = [sc / len(loader) for sc in metrics_score]\n",
        "    return epoch_loss, metrics_score\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    seeding(42)\n",
        "\n",
        "    create_dir(\"/content/drive/MyDrive/Farhana/RBVS_OCTA/chk_nbu/\") #For checkpoint\n",
        "\n",
        "    create_dir(\"/content/drive/MyDrive/Farhana/RBVS_OCTA/Result_nbu/\") # For CSV files\n",
        "\n",
        "    \"\"\" Load test dataset \"\"\"\n",
        "    data_dir=\"/content/drive/MyDrive/Farhana/RBVS_OCTA/Aug_OCTA/SVC/train\"\n",
        "\n",
        "    test_x = sorted(glob(\"/content/drive/MyDrive/Farhana/RBVS_OCTA/Aug_OCTA/SVC/test/image/*\"))\n",
        "    test_y = sorted(glob(\"/content/drive/MyDrive/Farhana/RBVS_OCTA/Aug_OCTA/SVC/test/mask/*\"))\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    num_epochs=50\n",
        "    batch_size=4\n",
        "    k=5\n",
        "    lr = 0.0005\n",
        "\n",
        "    checkpoint_path= \"/content/drive/MyDrive/Farhana/RBVS_OCTA/chk_nbu/chk02.pth\"\n",
        "    checkpoint_path2 = \"/content/drive/MyDrive/Farhana/RBVS_OCTA/chk_nbu/chk021.pth\"\n",
        "    checkpoint_path3 = \"/content/drive/MyDrive/Farhana/RBVS_OCTA/chk_nbu/chk022.pth\"\n",
        "\n",
        "\n",
        "    \"\"\" Dataset and loader \"\"\"\n",
        "\n",
        "    train_loader, valid_loader=get_train_valid_loader(data_dir=data_dir,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      random_seed=False,\n",
        "                                                      kfolde=k,\n",
        "                                                      shuffle=True,\n",
        "                                                      show_sample=False,\n",
        "                                                      num_workers=1,\n",
        "                                                      pin_memory=False,\n",
        "                                                      shuffelthevaluditation=1)\n",
        "    print(\"train data loader\",len(train_loader))\n",
        "    print(\"valid data loader\",len(valid_loader))\n",
        "\n",
        "    test_dataset=DriveDataset(test_x,test_y)\n",
        "    test_loader = DataLoader(dataset= test_dataset,batch_size=batch_size,shuffle=False,num_workers=1)\n",
        "    print(\"Test data loader\",len(test_loader))\n",
        "\n",
        "    loaded_checkpoint=torch.load(checkpoint_path)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = RIMNet(dropRate=0.00)\n",
        "    model = model.to(device)\n",
        "    print('Number of model parameters: {}'.format(\n",
        "      sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
        "    scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=5, power=0.9, last_epoch=- 1, verbose=True)\n",
        "\n",
        "    model.load_state_dict(loaded_checkpoint) #loading the trained model\n",
        "\n",
        "    loss_fn = DiceLoss().to(device)\n",
        "\n",
        "    \"\"\"opening/creating required csv files for recording data\"\"\"\n",
        "\n",
        "    with open('/content/drive/MyDrive/Farhana/RBVS_OCTA/Result_nbu/tr02.csv','a+') as td:\n",
        "        training_details=csv.writer(td)\n",
        "        with open('/content/drive/MyDrive/Farhana/RBVS_OCTA/Result_nbu/val02.csv','a+') as vd:\n",
        "            val_details=csv.writer(vd)\n",
        "            with open('/content/drive/MyDrive/Farhana/RBVS_OCTA/Result_nbu/te02.csv','a+') as ted:\n",
        "                test_details=csv.writer(ted)\n",
        "\n",
        "                training_details.writerow(['Epoch','TrLoss','Acc', 'FDR', 'Recall','Precision','F1', 'Specificity', 'Sensitivity',\n",
        "                                           'AUC','IoU', 'Dice_coeff', 'G_Mean', 'Kappa'])\n",
        "\n",
        "                val_details.writerow(['Epoch','ValLoss','Acc', 'FDR',  'Recall','Precision','F1', 'Specificity', 'Sensitivity',\n",
        "                                      'AUC','IoU', 'Dice_coeff','G_Mean', 'Kappa'])\n",
        "\n",
        "                test_details.writerow(['Epoch','TestLoss','Acc', 'FDR', 'Recall','Precision', 'F1', 'Specificity', 'Sensitivity',\n",
        "                                       'AUC','IoU', 'Dice_coeff','G_Mean', 'Kappa'])\n",
        "\n",
        "                \"\"\" Training the model \"\"\"\n",
        "                best_valid_loss = float(\"inf\")\n",
        "\n",
        "                best_tr_acc = 0.000\n",
        "                best_val_acc = 0.000\n",
        "                # acc, fdr, recall, precision,  f1, Specificity, Sensitivity, auc, iou, dice_coeff, g_mean, kappa_score\n",
        "                # 0     1     2       3          4      5             6         7   8     9           10      11\n",
        "                best_test_acc = 0.000\n",
        "                best_fdr = 0.000\n",
        "                best_recall = 0.000\n",
        "                best_precision = 0.000\n",
        "                best_f1 = 0.000\n",
        "                best_sp = 0.000\n",
        "                best_se = 0.000\n",
        "                best_auc = 0.000\n",
        "                best_iou = 0.000\n",
        "                best_dice = 0.00\n",
        "                best_g_mean = 0.00\n",
        "                best_kappa_score = 0.00\n",
        "\n",
        "                # Early stopping\n",
        "                patience = 10\n",
        "                trigger_times = 0\n",
        "\n",
        "\n",
        "                for epoch in range(num_epochs):\n",
        "                  start_time = time.time()\n",
        "                  train_loss, train_score= train(model, train_loader, optimizer, loss_fn, device)\n",
        "                  print(\"Training Done\")\n",
        "                  valid_loss, val_score = evaluate(model, valid_loader, loss_fn, device,'valid')\n",
        "                  print(\"Validation Done\")\n",
        "                  test_loss, test_score = test(model, test_loader, loss_fn, device,'valid')\n",
        "                  print(\"Test Done\")\n",
        "\n",
        "                  \"\"\" Saving the model \"\"\"\n",
        "                  if valid_loss <  best_valid_loss:\n",
        "                      data_str = f\"valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
        "                      print(data_str)\n",
        "                      best_valid_loss = valid_loss\n",
        "                      torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "                  else:\n",
        "                      trigger_times += 1\n",
        "                      print('Trigger Times:', trigger_times)\n",
        "\n",
        "                 ## '''\n",
        "                  if best_tr_acc < train_score[0]:\n",
        "                    best_tr_acc = max(train_score[0], best_tr_acc)\n",
        "\n",
        "                  if best_val_acc < val_score[0]:\n",
        "                    best_val_acc = max(val_score[0], best_val_acc)\n",
        "\n",
        "                  if best_test_acc < test_score[0]:\n",
        "                    best_test_acc = max(val_score[0], best_test_acc)\n",
        "\n",
        "                  if best_fdr > test_score[1]:\n",
        "                    best_fdr = min(test_score[1], best_fdr)\n",
        "\n",
        "                  if best_recall < test_score[2]:\n",
        "                    best_recall = max(test_score[2], best_recall)\n",
        "\n",
        "                  if best_precision < test_score[3]:\n",
        "                    best_precision = max(test_score[3], best_precision)\n",
        "\n",
        "                  if best_f1 < test_score[4]:\n",
        "                    best_f1 = max(test_score[4], best_f1)\n",
        "\n",
        "                  if best_sp < test_score[5]:\n",
        "                    best_sp = max(test_score[5], best_sp)\n",
        "\n",
        "                  if best_se < test_score[6]:\n",
        "                    best_se = max(test_score[6], best_se)\n",
        "\n",
        "                  if best_auc < test_score[7]:\n",
        "                    best_auc = max(test_score[7], best_auc)\n",
        "\n",
        "                  if best_iou < test_score[8]:\n",
        "                    best_iou = max(test_score[8], best_iou)\n",
        "\n",
        "                  if best_dice < test_score[9]:\n",
        "                    best_dice = max(test_score[9], best_dice)\n",
        "\n",
        "                  if best_g_mean < test_score[10]:\n",
        "                    best_g_mean = max(test_score[10], best_g_mean)\n",
        "\n",
        "                  if best_kappa_score < test_score[11]:\n",
        "                    best_kappa_score = max(test_score[11], best_kappa_score)\n",
        "\n",
        "                 # '''\n",
        "                  end_time = time.time()\n",
        "                  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "                  tacc = train_score[0]\n",
        "                  tjac = train_score[1]\n",
        "                  tf1 = train_score[2]\n",
        "                  trecall = train_score[3]\n",
        "                  tprecision = train_score[4]\n",
        "                  tsp = train_score[5]\n",
        "                  tse = train_score[6]\n",
        "                  tauc = train_score[7]\n",
        "                  tiou = train_score[8]\n",
        "                  tdice = train_score[9]\n",
        "                  tgmean = train_score[10]\n",
        "                  tkappa = train_score[11]\n",
        "\n",
        "                  vacc = val_score[0]\n",
        "                  vjac = val_score[1]\n",
        "                  vf1 = val_score[2]\n",
        "                  vrecall = val_score[3]\n",
        "                  vprecision = val_score[4]\n",
        "                  vsp = val_score[5]\n",
        "                  vse = val_score[6]\n",
        "                  vauc = val_score[7]\n",
        "                  viou = val_score[8]\n",
        "                  vdice = val_score[9]\n",
        "                  vgmean = val_score[10]\n",
        "                  vkappa = val_score[11]\n",
        "\n",
        "                  acc = test_score[0]\n",
        "                  jac = test_score[1]\n",
        "                  f1 = test_score[2]\n",
        "                  recall = test_score[3]\n",
        "                  precision = test_score[4]\n",
        "                  sp = test_score[5]\n",
        "                  se = test_score[6]\n",
        "                  auc = test_score[7]\n",
        "                  iou = test_score[8]\n",
        "                  dice = test_score[9]\n",
        "                  gmean = test_score[10]\n",
        "                  kappa = test_score[11]\n",
        "\n",
        "                  data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "                  data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "                  data_str += f'\\tVal. Loss: {valid_loss:.3f}'\n",
        "                  print(data_str)\n",
        "\n",
        "                  print(f\"\\tTrain: Mean ACC: {best_tr_acc:1.4f} \")\n",
        "                  print(f\"\\tVal: Mean Acc: {best_val_acc:1.4f} \")\n",
        "                  print(f\"\\tTest: Mean Acc: {best_test_acc:1.4f} \\t Mean IoU: {best_iou:1.4f} \\t Mean dice: {best_dice:1.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                  training_details.writerow([str(epoch+1),float(train_loss),float(tacc),float(tjac),float(tf1),float(trecall),float(tprecision),\n",
        "                                             float(tsp),float(tse),float(tauc),float(tiou),float(tdice),float(tgmean),float(tkappa)])\n",
        "                  val_details.writerow([str(epoch+1),float(valid_loss),float(vacc),float(vjac),float(vf1),float(vrecall),float(vprecision),\n",
        "                                        float(vsp),float(vse),float(vauc),float(viou),float(vdice),float(vgmean),float(vkappa)])\n",
        "                  test_details.writerow([str(epoch+1),float(test_loss),float(acc),float(jac),float(f1),float(recall),float(precision),\n",
        "                                         float(sp),float(se),float(auc),float(iou),float(dice),float(gmean),float(kappa)])\n",
        "\n",
        "                  if trigger_times >= patience:\n",
        "                    print('Early stopping!\\nStart to test process.')\n",
        "                    break\n",
        "\n",
        "                  train_loader, valid_loader = get_train_valid_loader(kfolde=k,shuffelthevaluditation=(epoch%k)+1,\n",
        "                                                                      random_seed=False,data_dir=data_dir,batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "                  #gc.collect()\n",
        "\n",
        "                  torch.cuda.empty_cache()\n",
        "                  #break\n",
        "\n",
        "    print(\"Completed Successfully\")\n",
        "    print(f\"Train Accuracy:{best_tr_acc:1.4f}\")\n",
        "    print(f\"Validation Accuracy:{best_val_acc:1.4f}\\n\")\n",
        "    print(\"Result on Test dataset\")\n",
        "    print(f\"Test Accuracy:{best_test_acc:1.4f}\")\n",
        "    print(f\"FDR:{best_fdr:1.4f}\")\n",
        "    print(f\"F1-Score:{best_f1:1.4f}\")\n",
        "    print(f\"Recall:{best_recall:1.4f}\")\n",
        "    print(f\"Precision:{best_precision:1.4f}\")\n",
        "    print(f\"Sensitivity:{best_se:1.4f}\")\n",
        "    print(f\"Specificity:{best_sp:1.4f}\")\n",
        "    print(f\"AUC:{best_auc:1.4f}\")\n",
        "    print(f\"IoU:{best_iou:1.4f}\")\n",
        "    print(f\"DC:{best_dice:1.4f}\")\n",
        "    print(f\"G Mean:{best_g_mean:1.4f}\")\n",
        "    print(f\"Kappa:{best_kappa_score:1.4f}\")\n"
      ]
    }
  ]
}